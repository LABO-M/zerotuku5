{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.VAEから拡散モデルへ\n",
    "\n",
    "DDPM : Denoising Diffusion Probabilistic Models を作る。\n",
    "\n",
    "### VAE の復習\n",
    "データ生成は、潜在変数を固定の正規分布からサンプリングし、潜在変数から観測変数への変換をNNで行う。また、観測変数から潜在変数への変換もNNを使う。\n",
    "\n",
    "![](./picture/pict1.png)\n",
    "\n",
    "### 階層型VAE（潜在変数の階層化）\n",
    "VAEの潜在変数をT個にしたときの階層型VAEは以下のようなモデル\n",
    "\n",
    "![](./picture/pict2.png)\n",
    "\n",
    "ここで示す階層型VAEは、直前の確率変数だけから決定されるモデル。各確率分布にはマルコフ性を仮定し、パラメータの増加を防ぐ。階層化によって複雑な表現を効率的に表せる。\n",
    "\n",
    "しかし、問題点がある。すべてで２T個の確率分布があり、デコーダが２T個、エンコーダ２T個の計2T個のNNが必要になる。T=1000等になるとこの手法では厳しい。\n",
    "\n",
    "→T=1000でも１つのNNでモデル化できる**拡散モデル**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 拡散モデルへ\n",
    "階層型VAEで２つの点を変更すれば、「拡散モデル」になる。\n",
    "\n",
    "１，観測変数と潜在変数の次元数は同じ<br>\n",
    "２，エンコーダは、固定の正規分布のノイズを追加する\n",
    "\n",
    "![](./picture/pict3.png)\n",
    "\n",
    "$x_{0}$が観測変数でそれ以外は潜在変数。潜在変数と観測変数の次元数が同じなので、すべて$x$で統一する。<br>\n",
    "ノイズを追加する過程（拡散過程）は固定のガウスノイズを追加するだけなので、$\\phi$のパラメータは不要。\n",
    "\n",
    "**拡散モデルは、ノイズを除去する処理をNNでモデル化します**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.拡散過程と逆拡散過程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 拡散過程\n",
    "拡散過程は、最終時刻Tにおいて「完全なノイズ」になるように前時刻のデータにノイズを加えます。\n",
    "\n",
    "![](./picture/pict4.png)\n",
    "\n",
    "$x_{T} \\sim N(x_{T}; 0,I)$ に従うように、各時刻で加えるノイズを調節します。時刻ｔで以下のようにノイズを加える。\n",
    "\n",
    "![](./picture/pict5.png)\n",
    "\n",
    "$t$は$1\\le t \\le T$とした整数で、　$\\beta _{t}$ はハイパーパラメータとする。（例えば０．０１）<br>\n",
    "\n",
    "$\\beta _{t}$を大きくすると加えるノイズが大きくなる。<br>\n",
    "\n",
    "Tを大きくして、各時刻ごとに適切な$\\beta _{t}$を設定し、（全てでT個）することで、$p(x_{T}) \\thickapprox N(x_{T};0,I)$ となる。\n",
    "\n",
    "逆伝播を行うための変数変換トリックは以下の通り。\n",
    "![](./picture/pict6.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 逆拡散過程\n",
    "\n",
    "ノイズを除去する処理の部分。NNで行う。\n",
    "![](./picture/pict7.png)\n",
    "\n",
    "全てでT回ノイズ除去を行う必要がある。\n",
    "\n",
    "個別にNNを使う方法もあるが、拡散モデルでは変数の次元が全て同じなので、各時刻で同じ「１つのNN」で実装できそう。\n",
    "\n",
    "どの時刻の処理かを区別するために、時刻$t$を入力信号として与える。\n",
    "\n",
    "![](./picture/pict8.png)\n",
    "\n",
    "NNは、ノイズを少しだけ除去した画像を$\\hat{x}_{t-1}$として出力する。これは推定値であり、時刻ｔのデータから、t-1のデータを推測したという意味。\n",
    "\n",
    "$p_{\\theta}(x_{t-1}|x_{t})$をNNを使ってモデル化する必要がある。ここでは、推定値を平均とする正規分布としてモデル化する。\n",
    "\n",
    "![](./picture/pict9.png)\n",
    "\n",
    "構成はできたので次は拡散モデルの学習方法について考える"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.ELBOの計算①"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "やることは、VAEのときと同じで、対数尤度の代わりにELBOを大きくすることを考える。<br>\n",
    "拡散モデルのELBOを実際に計算するは厳しいので、ELBOを近似して計算することを目指す。\n",
    "\n",
    "#### ①サンプルサイズT\n",
    "T個のサンプリングデータを使ってELBOを近似する\n",
    "\n",
    "まずは、VAEのELBOの復習\n",
    "![](./picture/pict10.png)\n",
    "\n",
    "変更点は３つ\n",
    "\n",
    "![](./picture/pict11.png)\n",
    "\n",
    "よって、拡散モデルのELBOは\n",
    "\n",
    "![](./picture/pict13.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "この式をもとに、ELBOの式展開をしていく。\n",
    "\n",
    "![](./picture/pict14.png)\n",
    "\n",
    "最初のイコールはマルコフ性から。パラメータ$\\theta$に依存しているところに注目して式変形。\n",
    "\n",
    "期待値はモンテカルロ法によって近似できるので、$x_{1:T}$をいくつか生成して期待値の中身の平均を求めればよい。例えばモンテカルロ法のサンプルサイズを１とすると、ELBOの近似は次の式\n",
    "\n",
    "![](./picture/pict16.png)\n",
    "\n",
    "先程のモデル化の部分を思い出して、式変形していくと、\n",
    "\n",
    "![](./picture/pict17.png)\n",
    "\n",
    "よって、目的関数$J(\\theta)$は、\n",
    "\n",
    "$$J(\\theta)=-1/2\\sum_{t=0}^{T-1}\\|x_{t}-\\hat x_{t}\\|^{2}$$\n",
    "\n",
    "サンプルサイズTでの目的関数Jの求め方は、\n",
    "\n",
    "１ 拡散過程によりT個のサンプリングを行い<br>\n",
    "２ NNをT回適用してノイズ除去を行い<br>\n",
    "３ 各時刻の二乗誤差$\\|x_{t}-\\hat x_{t}\\|^{2}$を求める\n",
    "\n",
    "次の方法は、T回サンプリングしなくてもよい方法について見ていく"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.ELBOの計算②\n",
    "\n",
    "#### ②サンプルサイズ２\n",
    "拡散過程の性質を利用して、元データに対して１度ノイズを追加することで、任意の時刻tの$x_{t}$をサンプリングする。つまり、$q(x_{t}|x_{0})$を解析的に表す。\n",
    "\n",
    "![](./picture/pict18.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "先に結論\n",
    "\n",
    "$q(x_{t}|x_{0})$は次の式で表現できる\n",
    "\n",
    "![](./picture/pict19.png)\n",
    "\n",
    "(pf)\n",
    "まず拡散過程は次の式だった\n",
    "$$\n",
    "q(x_t \\mid x_{t-1}) = \\mathcal{N}(x_t; \\sqrt{1 - \\beta_t} x_{t-1}, \\beta_t \\mathbf{I})\n",
    "$$\n",
    "\n",
    "ここでは、次の $ \\alpha_t $ の記号を使い $ q(x_t \\mid x_{t-1}) $ を表します。\n",
    "\n",
    "$$\n",
    "\\alpha_t = 1 - \\beta_t\n",
    "$$\n",
    "\n",
    "$$\n",
    "q(x_t \\mid x_{t-1}) = \\mathcal{N}(x_t; \\sqrt{\\alpha_t} x_{t-1}, (1 - \\alpha_t) \\mathbf{I})\n",
    "$$\n",
    "\n",
    "\n",
    "このとき $q(x_t \\mid x_{t-1}) $からのサンプリングは、変数変換トリックを使うと次の式で表されます。\n",
    "\n",
    "$$\n",
    "\\varepsilon_t \\sim \\mathcal{N}(0, \\mathbf{I})\n",
    "$$\n",
    "\n",
    "$$\n",
    "x_t = \\sqrt{\\alpha_t} x_{t-1} + \\sqrt{1 - \\alpha_t} \\varepsilon_t \\tag{8.5}\n",
    "$$\n",
    "\n",
    "時刻 $ t $における$\\varepsilon_t$は、標準正規分布からサンプリングされます。続いて、式 (8.5) の $ t $に$ t-1$ を代入して次の式を得ます。\n",
    "\n",
    "$$\n",
    "\\varepsilon_{t-1} \\sim \\mathcal{N}(0, \\mathbf{I})\n",
    "$$\n",
    "\n",
    "$$\n",
    "x_{t-1} = \\sqrt{\\alpha_{t-1}} x_{t-2} + \\sqrt{1 - \\alpha_{t-1}} \\varepsilon_{t-1} \\tag{8.6}\n",
    "$$\n",
    "\n",
    "$$\n",
    "x_t = \\sqrt{\\alpha_t} x_{t-1} + \\sqrt{1 - \\alpha_t} \\varepsilon_t\n",
    "$$\n",
    "\n",
    "これを（８．５）式に代入すると\n",
    "$$\n",
    "= \\sqrt{\\alpha_t} \\left( \\sqrt{\\alpha_{t-1}} x_{t-2} + \\sqrt{1 - \\alpha_{t-1}} \\varepsilon_{t-1} \\right) + \\sqrt{1 - \\alpha_t} \\varepsilon_t\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\sqrt{\\alpha_t \\alpha_{t-1}} x_{t-2} + \\sqrt{\\alpha_t (1 - \\alpha_{t-1})} \\varepsilon_{t-1} + \\sqrt{1 - \\alpha_t} \\varepsilon_t \\tag{8.7}\n",
    "$$\n",
    "\n",
    "ガウスノイズ項は１つにまとめることができて、\n",
    "$$\n",
    "\\varepsilon \\sim \\mathcal{N}(0, \\mathbf{I})\n",
    "$$\n",
    "\n",
    "$$\n",
    "x_t = \\sqrt{\\alpha_t \\alpha_{t-1}} x_{t-2} + \\sqrt{1 - \\alpha_t \\alpha_{t-1}} \\varepsilon \\tag{8.8}\n",
    "$$\n",
    "この操作を$x_{0}$がでるまで続けると、\n",
    "\n",
    "$$\n",
    "x_t = \\sqrt{\\alpha_t \\alpha_{t-1} \\cdots \\alpha_1} x_0 + \\sqrt{1 - \\alpha_t \\alpha_{t-1} \\cdots \\alpha_1} \\, \\varepsilon\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\sqrt{\\bar{\\alpha}_t} x_0 + \\sqrt{1 - \\bar{\\alpha}_t} \\, \\varepsilon \\tag{8.9}\n",
    "$$\n",
    "\n",
    "ここでは次のように $ \\alpha_t $ から $\\alpha_1$ までの積を $\\bar{\\alpha}_t $で表すことにします。\n",
    "\n",
    "$$\n",
    "\\bar{\\alpha}_t = \\alpha_t \\alpha_{t-1} \\cdots \\alpha_1\n",
    "$$\n",
    "よって成り立つ。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これを利用して、先程の目的関数Jを式変形する\n",
    "\n",
    "![](./picture/pict20.png)\n",
    "\n",
    "２行目の式変形は期待値の線型性、３行目の式変形は期待値を取る変数のみ分布から取り出せればいいので、必要ない変数を含む分布から取り出さなくても良い。\n",
    "\n",
    "この式で重要な点は２つの手順でサンプリングできるということ\n",
    "\n",
    "![](./picture/pict21.png)\n",
    "\n",
    "目的関数JはT個の和として表現されている。Tが多いと計算量が大きくなるので、「T個の和」を近似したい→**一様分布に関する期待値**\n",
    "\n",
    "\n",
    "\n",
    "$$\n",
    "\\mathbb{E}_{u(t)}\\left[ f(t) \\right] = \\sum_{t=1}^T u(t) f(t) = \\sum_{t=1}^T \\frac{1}{T} f(t) = \\frac{1}{T} \\sum_{t=1}^T f(t)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\sum_{t=1}^T f(t) = T \\mathbb{E}_{u(t)}\\left[ f(t) \\right]\n",
    "$$\n",
    "\n",
    "この近似を利用すると、\n",
    "\n",
    "![](./picture/pict22.png)\n",
    "\n",
    "二重の期待値は次のようなサンプリングで近似できます。例えばモンテカルロ法のサンプルサイズを１とすると、\n",
    "\n",
    "$$\n",
    "t \\sim U\\{1, T\\}\n",
    "$$\n",
    "\n",
    "$$\n",
    "x_{t-1} \\sim q(x_{t-1} \\mid x_0)\n",
    "$$\n",
    "\n",
    "$$\n",
    "x_t \\sim q(x_t \\mid x_{t-1})\n",
    "$$\n",
    "\n",
    "$$\n",
    "J(\\theta) \\approx T \\log p_{\\theta}(x_{t-1} \\mid x_t)\n",
    "$$\n",
    "\n",
    "なお、$p_{\\theta}(x_{t-1} \\mid x_t)$ は次の式で表されます。\n",
    "\n",
    "$$\n",
    "\\hat{x}_{t-1} = \\text{NeuralNet}(x_t, t; \\theta)\n",
    "$$\n",
    "\n",
    "$$\n",
    "p_{\\theta}(x_{t-1} \\mid x_t) = \\mathcal{N}(x_{t-1}; \\hat{x}_{t-1}, \\mathbf{I})\n",
    "$$\n",
    "\n",
    "サンプルサイズTのところでも見たように、正規分布の対数尤度は２乗誤差に帰着するので、$J(\\theta)$は\n",
    "\n",
    "$$J(\\theta) \\approx -T/2\\|x_{t}-\\hat x_{t}\\|^{2}$$\n",
    "\n",
    "まとめると、サンプルサイズ２の目的関数の計算は\n",
    "\n",
    "1 一様分布U(1,T)から時刻tサンプリングする<br>\n",
    "2　$q(x_{t-1}|x_{0})$から$x_{t-1}$をサンプリングし、次に$q(x_{t}|x_{t-1})$から$x_{t}$をサンプリングする<br>\n",
    "3　NNに$x_{t}$を入力して、$\\hat x_{t-1}$ を出力する<br>\n",
    "4　２乗誤差$\\|x_{t-1}-\\hat x_{t-1}\\|^{2}$を求める<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.ELBOの計算③\n",
    "\n",
    "ここでは、サンプルサイズが１でもELBOを近似する方法について考えます。\n",
    "\n",
    "keyになるのが、$q(x_{t-1}|x_{t},x_{0})$ という確率分布\n",
    "\n",
    "![](./picture/pict23.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ q(x_{t-1} \\mid x_t, x_0) $ は、数式では次のように表されます。\n",
    "\n",
    "$$\n",
    "q(x_{t-1} \\mid x_t, x_0) = \\mathcal{N}(x_{t-1}; \\mu_q(x_t, x_0), \\sigma_q^2(t) \\mathbf{I}) \\tag{8.10}\n",
    "$$\n",
    "\n",
    "ここでは以下の記号を使用します。\n",
    "\n",
    "$$\n",
    "\\alpha_t = 1 - \\beta_t\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\bar{\\alpha}_t = \\alpha_t \\alpha_{t-1} \\cdots \\alpha_1\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mu_q(x_t, x_0) = \\frac{\\sqrt{\\alpha_t (1 - \\bar{\\alpha}_{t-1})} x_t + \\sqrt{\\bar{\\alpha}_{t-1} (1 - \\alpha_t)} x_0}{1 - \\bar{\\alpha}_t}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\sigma_q^2(t) = \\frac{(1 - \\alpha_t)(1 - \\bar{\\alpha}_{t-1})}{1 - \\bar{\\alpha}_t}\n",
    "$$\n",
    "\n",
    "![](./picture/pict24.png)\n",
    "平均ベクトルは上の図のように２点の内分点（ようなもの）として理解できる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(pf)\n",
    "$ q(x_{t-1} \\mid x_t, x_0) $ の導出を行います。$ q(x_{t-1} \\mid x_t, x_0) $ はベイズの定理より次の等式が成り立ちます。\n",
    "\n",
    "$$\n",
    "q(x_{t-1} \\mid x_t, x_0) = \\frac{q(x_t \\mid x_{t-1}, x_0) \\, q(x_{t-1} \\mid x_0)}{q(x_t \\mid x_0)}\n",
    "$$\n",
    "\n",
    "ここでマルコフ性により $q(x_t \\mid x_{t-1}, x_0) = q(x_t \\mid x_{t-1}) $ が成り立ちます。\n",
    "\n",
    "よって、$ q(x_{t-1} \\mid x_t, x_0) $ は次の式で表されます。\n",
    "\n",
    "$$\n",
    "q(x_{t-1} \\mid x_t, x_0) = \\frac{q(x_t \\mid x_{t-1}) \\, q(x_{t-1} \\mid x_0)}{q(x_t \\mid x_0)}\n",
    "$$\n",
    "\n",
    "さて、拡散過程に関する確率分布について、これまで得た式は次のとおりです。\n",
    "\n",
    "$$\n",
    "q(x_t \\mid x_{t-1}) = \\mathcal{N}(x_t; \\sqrt{\\alpha_t} x_{t-1}, (1 - \\alpha_t) \\mathbf{I})\n",
    "$$\n",
    "\n",
    "$$\n",
    "q(x_t \\mid x_0) = \\mathcal{N}(x_t; \\sqrt{\\bar{\\alpha}_t} x_0, (1 - \\bar{\\alpha}_t) \\mathbf{I})\n",
    "$$\n",
    "\n",
    "これより $ q(x_{t-1} \\mid x_t, x_0) $ は次のように式展開できます。\n",
    "\n",
    "$$\n",
    "q(x_{t-1} \\mid x_t, x_0) = \\frac{\\mathcal{N}(x_t; \\sqrt{\\alpha_t} x_{t-1}, (1 - \\alpha_t) \\mathbf{I}) \\, \\mathcal{N}(x_{t-1}; \\sqrt{\\bar{\\alpha}_{t-1}} x_0, (1 - \\bar{\\alpha}_{t-1}) \\mathbf{I})}{\\mathcal{N}(x_t; \\sqrt{\\bar{\\alpha}_t} x_0, (1 - \\bar{\\alpha}_t) \\mathbf{I})}\n",
    "$$\n",
    "\n",
    "上の式のとおり、3つの正規分布の積と除算によって表されます。\n",
    "そして、結果的に得られる $ q(x_{t-1} \\mid x_t, x_0) $ もまた正規分布になります。\n",
    "\n",
    "続いて、$q(x_{t-1} \\mid x_t, x_0) $ が「どのような正規分布になるか」を計算します。このような場合は、正規分布の指数部分に注目することで、効率的に式展開ができます。たとえば、\n",
    "\n",
    "$$\n",
    "\\mathcal{N}(x_{t-1}; \\sqrt{\\alpha_{t-1}} x_0, (1 - \\alpha_{t-1}) \\mathbf{I})\n",
    "$$\n",
    "\n",
    "の指数部分は次の式で表されます。\n",
    "\n",
    "$$\n",
    "\\mathcal{N}(x_t; \\sqrt{\\alpha_t} x_{t-1}, (1 - \\alpha_t) \\mathbf{I})\n",
    "\\propto \\exp \\left\\{ -\\frac{1}{2} \\left( x_t - \\sqrt{\\alpha_t} x_{t-1} \\right)^{\\top} ((1 - \\alpha_t) \\mathbf{I})^{-1} \\left( x_t - \\sqrt{\\alpha_t} x_{t-1} \\right) \\right\\}\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\exp \\left\\{ -\\frac{1}{2} \\frac{\\| x_t - \\sqrt{\\alpha_t} x_{t-1} \\|^2}{1 - \\alpha_t} \\right\\}\n",
    "$$\n",
    "\n",
    "ここで、$ \\propto $ は「比例する」を表す記号です。以上の点を踏まえて$ q(x_{t-1} \\mid x_t, x_0) $ の指数部分に注目すると次の式展開ができます。\n",
    "\n",
    "$$\n",
    "q(x_{t-1} \\mid x_t, x_0) \\propto \\exp \\left( -\\frac{1}{2} \\left( \\frac{\\| x_t - \\sqrt{\\alpha_t} x_{t-1} \\|^2}{1 - \\alpha_t} + \\frac{\\| x_{t-1} - \\sqrt{\\bar{\\alpha}_{t-1}} x_0 \\|^2}{1 - \\bar{\\alpha}_{t-1}} - \\frac{\\| x_t - \\sqrt{\\bar{\\alpha}_t} x_0 \\|^2}{1 - \\bar{\\alpha}_t} \\right) \\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\exp \\left( -\\frac{1}{2} \\left( \\frac{\\| x_{t-1} \\|^2 - 2 \\sqrt{\\alpha_t} x_t \\cdot x_{t-1} + \\alpha_t \\| x_t \\|^2}{1 - \\alpha_t} + \\frac{\\| x_{t-1} \\|^2 - 2 \\sqrt{\\bar{\\alpha}_{t-1}} x_0 \\cdot x_{t-1} + \\bar{\\alpha}_{t-1} \\| x_0 \\|^2}{1 - \\bar{\\alpha}_{t-1}} \\right) \\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\exp \\left( -\\frac{1}{2} \\left( \\left( \\frac{\\alpha_t}{1 - \\alpha_t} + \\frac{1}{1 - \\bar{\\alpha}_{t-1}} \\right) \\| x_{t-1} \\|^2 - 2 \\left( \\frac{\\sqrt{\\alpha_t}}{1 - \\alpha_t} x_t + \\frac{\\sqrt{\\bar{\\alpha}_{t-1}}}{1 - \\bar{\\alpha}_{t-1}} x_0 \\right) \\cdot x_{t-1} \\right) \\right) + C(x_t, x_0)\n",
    "$$\n",
    "\n",
    "ここで、$q(x_{t-1} \\mid x_t, x_0) $ は $ x_t = 1 $ の条件付き確率分布です。ここで $ C(x_t, x_0) $ は $ x_{t-1} $ に関係しない項を表します。続いて、上の式を\n",
    "\n",
    "$$\n",
    "\\exp \\left( -\\frac{1}{2} \\frac{\\| x_{t-1} - \\mu_q(x_t, x_0) \\|^2}{\\sigma_q^2(t)} \\right)\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "\\sigma_q^2(t) = \\frac{1}{\\left( \\frac{\\alpha_t}{1 - \\alpha_t} + \\frac{1}{1 - \\bar{\\alpha}_{t-1}} \\right)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\frac{(1 - \\alpha_t)(1 - \\bar{\\alpha}_{t-1})}{(1 - \\bar{\\alpha}_{t-1}) \\alpha_t + (1 - \\alpha_t)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\frac{(1 - \\alpha_t)(1 - \\bar{\\alpha}_{t-1})}{1 - \\bar{\\alpha}_t}\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "\\mu_q(x_t, x_0) = \\frac{\\sqrt{\\alpha_t} \\, x_t + \\sqrt{\\bar{\\alpha}_{t-1}} (1 - \\alpha_t) \\, x_0}{1 - \\bar{\\alpha}_t}\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\frac{\\sqrt{\\alpha_t (1 - \\bar{\\alpha}_{t-1})} \\, x_t + \\sqrt{\\bar{\\alpha}_{t-1} (1 - \\alpha_t)} \\, x_0}{1 - \\bar{\\alpha}_t}\n",
    "$$\n",
    "\n",
    "これより次の式を得ることができました。\n",
    "\n",
    "$$\n",
    "q(x_{t-1} \\mid x_t, x_0) = \\mathcal{N}\\left(x_{t-1}; \\frac{\\sqrt{\\alpha_t (1 - \\bar{\\alpha}_{t-1})} \\, x_t + \\sqrt{\\bar{\\alpha}_{t-1} (1 - \\alpha_t)} \\, x_0}{1 - \\bar{\\alpha}_t}, \\frac{(1 - \\alpha_t)(1 - \\bar{\\alpha}_{t-1})}{1 - \\bar{\\alpha}_t} \\mathbf{I} \\right)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上の式を利用して、サンプルサイズ１のELBOの近似解を求める\n",
    "\n",
    "目的関数は以下のように式変形できた。\n",
    "\n",
    "$$\n",
    "J(\\theta) = \\sum_{t=1}^T \\mathbb{E}_{q(x_{t-1}, x_t \\mid x_0)} \\left[ \\log p_{\\theta}(x_{t-1} \\mid x_t) \\right]\n",
    "$$\n",
    "\n",
    "$$\n",
    "= T \\, \\mathbb{E}_{u(t)} \\left[ J_0 \\right]\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "ここでは、$ J_0 $ = $\\mathbb{E}_{q(x_{t-1}, x_t \\mid x_0)} \\left[ \\log p_{\\theta}(x_{t-1} \\mid x_t) \\right]$\n",
    "\n",
    "\n",
    "目的関数は $ T \\, \\mathbb{E}_{u(t)}[J_0] $ となります。$\\mathbb{E}_{u(t)}[\\cdot] $ はモンテカルロ法によって近似できます。\n",
    "定数項を加えても目的関数を最大化する$\\theta$は変わらないので、次の定数項をわざと加えます。\n",
    "\n",
    "$$\n",
    "\\arg \\max_{\\theta} J_0 = \\arg \\max_{\\theta} \\left( J_0 - \\mathbb{E}_{q(x_{t-1}, x_t \\mid x_0)} \\left[ \\log q(x_{t-1} \\mid x_t, x_0) \\right] \\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\arg \\max_{\\theta} \\mathbb{E}_{q(x_{t-1}, x_t \\mid x_0)} \\left[ \\log p_{\\theta}(x_{t-1} \\mid x_t) - \\log q(x_{t-1} \\mid x_t, x_0) \\right]\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\arg \\max_{\\theta} \\mathbb{E}_{q(x_{t-1}, x_t \\mid x_0)} \\left[ \\log \\frac{p_{\\theta}(x_{t-1} \\mid x_t)}{q(x_{t-1} \\mid x_t, x_0)} \\right]\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ J_1 = \\mathbb{E}_{q(x_{t-1}, x_t \\mid x_0)} \\left[ \\log \\frac{p_{\\theta}(x_{t-1} \\mid x_t)}{q(x_{t-1} \\mid x_t, x_0)} \\right]$とすると、\n",
    "\n",
    "$ J_0 $ と $ J_1 $ は最大となる $ \\theta $ が同じです。そのため、次の $ J_1 $ を使った式を目的関数にできます。\n",
    "\n",
    "$$\n",
    "J(\\theta) = T \\, \\mathbb{E}_{u(t)}[J_1]\n",
    "$$\n",
    "\n",
    "続いて、$ J_1 $ を次のように式展開します。\n",
    "\n",
    "$$\n",
    "J_1 = \\int q(x_{t-1}, x_t \\mid x_0) \\log \\frac{p_{\\theta}(x_{t-1} \\mid x_t)}{q(x_{t-1} \\mid x_t, x_0)} \\, dx_{t-1} \\, dx_t\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\int q(x_t \\mid x_0) \\, q(x_{t-1} \\mid x_t, x_0) \\log \\frac{q(x_{t-1} \\mid x_t, x_0)}{p_{\\theta}(x_{t-1} \\mid x_t)} \\, dx_{t-1} \\, dx_t\n",
    "$$\n",
    "\n",
    "$$\n",
    "= -\\mathbb{E}_{q(x_t \\mid x_0)} \\left[ D_{KL}(q(x_{t-1} \\mid x_t, x_0) \\parallel p_{\\theta}(x_{t-1} \\mid x_t)) \\right]\n",
    "$$\n",
    "\n",
    "ここでは確率の乗法定理を使って展開します。最終的に $ J_1 $ は、$q(x_t \\mid x_0) $ に関する KL ダイバージェンスの期待値として表されます。モンテカルロ法を使えば、$ x_t $ のサンプル 1 つだけで近似できます。\n",
    "\n",
    "また、KL ダイバージェンスは 2 つの確率分布が等しいときに最小値（= 0）を取ります。$ J_1 $ は「マイナスの KL ダイバージェンス」なので、2 つの確率分布が等しい $ p_{\\theta}(x_{t-1} \\mid x_t) = q(x_{t-1} \\mid x_t, x_0) $ のときに最大になります。\n",
    "\n",
    "つまり、ここでの目標は、$ p_{\\theta}(x_{t-1} \\mid x_t) $ を $ q(x_{t-1} \\mid x_t, x_0) $ に一致させることです。$ q(x_{t-1} \\mid x_t, x_0) $ は、式 (8.10) のとおり正規分布として表されることを思い出します。\n",
    "\n",
    "そのため、$p_{\\theta}(x_{t-1} \\mid x_t) $ を次のように正規分布としてモデル化します。\n",
    "\n",
    "$$\n",
    "\\hat{x}_{t-1} = \\text{NeuralNet}(x_t, t; \\theta)\n",
    "$$\n",
    "\n",
    "$$\n",
    "p_{\\theta}(x_{t-1} \\mid x_t) = \\mathcal{N}(x_{t-1}; \\hat{x}_{t-1}, \\sigma_q^2(t) \\mathbf{I})\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "$ p_{\\theta}(x_{t-1} \\mid x_t) $ の平均ベクトルは、NNによって求めます。\n",
    "\n",
    "そして $ p_{\\theta}(x_{t-1} \\mid x_t) $ の共分散行列は、$ q(x_{t-1} \\mid x_t, x_0) $ と同じ値$ \\sigma_q^2(t) \\mathbf{I} $ に設定します。\n",
    "\n",
    "なお、これ以降はNNを $ \\mu_{\\theta}(x_t, t) $ と簡略化して表すことにします。これにより、次の式で $p_{\\theta}(x_{t-1} \\mid x_t) $ を表します。\n",
    "\n",
    "$$\n",
    "p_{\\theta}(x_{t-1} \\mid x_t) = \\mathcal{N}(x_{t-1}; \\mu_{\\theta}(x_t, t), \\sigma_q^2(t) \\mathbf{I})\n",
    "$$\n",
    "\n",
    "次に KL ダイバージェンスの計算です。今回は 2 つの正規分布の KL ダイバージェンスであり、これは次の式により解析的に求められます\n",
    "(7章のコラム、２つの正規分布間のKLダイバージェンスを参照)\n",
    "$$\n",
    "D_{KL}(q(x_{t-1} \\mid x_t, x_0) \\parallel p_{\\theta}(x_{t-1} \\mid x_t)) = \\frac{1}{2 \\sigma_q^2(t)} \\| \\mu_{\\theta}(x_t, t) - \\mu_q(x_t, x_0) \\|^2\n",
    "$$\n",
    "\n",
    "以上をまとめると、目的関数は次の式で表されます。\n",
    "\n",
    "$$\n",
    "J(\\theta) = -T \\, \\mathbb{E}_{u(t)} \\left[ \\mathbb{E}_{q(x_{t-1}, x_t \\mid x_0)} \\left[ \\frac{1}{2 \\sigma_q^2(t)} \\| \\mu_{\\theta}(x_t, t) - \\mu_q(x_t, x_0) \\|^2 \\right] \\right]\n",
    "$$\n",
    "\n",
    "損失関数は目的関数にマイナスを付けることで求められます。また、損失関数を定数倍にすることで、オプティマイザの学習率の設定で調整できます。そこで上の式を -$\\frac{1}{2}$ 倍した値を損失関数に設定します。\n",
    "\n",
    "$$\n",
    "\\text{LOSS}(x_0; \\theta) = \\mathbb{E}_{u(t)} \\left[ \\mathbb{E}_{q(x_{t-1}, x_t \\mid x_0)} \\left[ \\frac{1}{\\sigma_q^2(t)} \\| \\mu_{\\theta}(x_t, t) - \\mu_q(x_t, x_0) \\|^2 \\right] \\right]\n",
    "$$\n",
    "\n",
    "以上が損失関数の計算方法です。モンテカルロ法（サンプルサイズを1とする）を使うと、損失関数は次のように求められます。\n",
    "\n",
    "$$\n",
    "t \\sim U\\{1, T\\}\n",
    "$$\n",
    "\n",
    "$$\n",
    "x_t \\sim q(x_t \\mid x_0)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{LOSS}(x_0; \\theta) = \\frac{1}{\\sigma_q^2(t)} \\| \\mu_{\\theta}(x_t, t) - \\mu_q(x_t, x_0) \\|^2\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このように、損失関数は２乗誤差として表現できる。NNでは $\\mu_{\\theta}(x_t, t)$ が $\\mu_q(x_t, x_0) $　に近づくようにパラメータが学習される"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.拡散モデルの学習\n",
    "\n",
    "![](./picture/pict25.png)\n",
    "\n",
    "では今回作ったNNでは何を出力するモデルになっているのでしょうか。\n",
    "\n",
    "![](./picture/pict26.png)\n",
    "\n",
    "今回のモデルでは、ノイズが少し除去された画像を出力としている。\n",
    "\n",
    "NNはこれ以外の構成方法も考えられる。\n",
    "\n",
    "### 元データを復元するNN\n",
    "\n",
    "時刻ｔでの平均ベクトルは以下のようになっていた。\n",
    "$$\n",
    "\\mu_q(x_t, x_0) = \\frac{\\sqrt{\\alpha_t (1 - \\bar{\\alpha}_{t-1})} \\, x_t + \\sqrt{\\bar{\\alpha}_{t-1} (1 - \\alpha_t)} \\, x_0}{1 - \\bar{\\alpha}_t} \\tag{8.11}\n",
    "$$\n",
    "\n",
    "そして $ \\mu_{\\theta}(x_t, t) $ をこの式の形式に合わせるように調整します。それが次の式です。\n",
    "\n",
    "$$\n",
    "\\mu_{\\theta}(x_t, t) = \\frac{\\sqrt{\\alpha_t (1 - \\bar{\\alpha}_{t-1})} \\, x_t + \\sqrt{\\bar{\\alpha}_{t-1} (1 - \\alpha_t)} \\, \\hat{x}_{\\theta}(x_t, t)}{1 - \\bar{\\alpha}_t} \\tag{8.12}\n",
    "$$\n",
    "\n",
    "先程までは8.11を予測するNNを作成していた。このようにすると、損失関数つまり、KLDは以下のようになる\n",
    "\n",
    "$$\n",
    "D_{KL}(q(x_{t-1} \\mid x_t, x_0) \\parallel p_{\\theta}(x_{t-1} \\mid x_t)) \n",
    "= \\frac{1}{2 \\sigma_q^2(t)} \\| \\mu_{\\theta}(x_t, t) - \\mu_q(x_t, x_0) \\|^2\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\frac{1}{2 \\sigma_q^2(t)} \\left( \\frac{\\sqrt{\\bar{\\alpha}_{t-1} (1 - \\alpha_t)}}{1 - \\bar{\\alpha}_t} \\right)^2 \\| \\hat{x}_{\\theta}(x_t, t) - x_0 \\|^2\n",
    "$$\n",
    "\n",
    "KL ダイバージェンスが損失関数となるので、上の式より、損失関数は $\\hat{x}_{\\theta}(x_t, t) $ と $ x_0 $ の 2 乗誤差として表されます。そのため、$ \\hat{x}_{\\theta}(x_t, t) $ は $ x_0 $ を「教師データ」として、それと同じ出力になるように学習します。つまり今回のNNで行う処理は、下図のようになる\n",
    "\n",
    "![](./picture/pict27.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ノイズを予測するNN\n",
    "$q(x_t \\mid x_0)$を思い出すと、\n",
    "\n",
    "$$\n",
    "q(x_t \\mid x_0) = \\mathcal{N}(x_t; \\sqrt{\\bar{\\alpha}_t} x_0, (1 - \\bar{\\alpha}_t) \\mathbf{I})\n",
    "$$\n",
    "\n",
    "そして $q(x_t \\mid x_0) $ からのサンプルは、次の式で表されることを利用します。\n",
    "\n",
    "$$\n",
    "\\varepsilon \\sim \\mathcal{N}(0, \\mathbf{I})\n",
    "$$\n",
    "\n",
    "$$\n",
    "x_t = \\sqrt{\\bar{\\alpha}_t} x_0 + \\sqrt{1 - \\bar{\\alpha}_t} \\, \\varepsilon\n",
    "$$\n",
    "\n",
    "これより式変形すると、\n",
    "\n",
    "$$\n",
    "x_0 = \\frac{x_t - \\sqrt{1 - \\bar{\\alpha}_t} \\, \\varepsilon}{\\sqrt{\\bar{\\alpha}_t}}\n",
    "$$\n",
    "\n",
    "そして、上の式を式 (8.11) に代入して式展開します。\n",
    "\n",
    "$$\n",
    "\\mu_q(x_t, x_0) = \\frac{\\sqrt{\\alpha_t (1 - \\bar{\\alpha}_{t-1})} \\, x_t + \\sqrt{\\bar{\\alpha}_{t-1} (1 - \\alpha_t)} \\, x_0}{1 - \\bar{\\alpha}_t}\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\frac{1}{\\sqrt{\\alpha_t}} \\left( x_t - \\frac{1 - \\alpha_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\varepsilon \\right)\n",
    "$$\n",
    "\n",
    "この式に対応させるように$ \\mu_{\\theta}(x_t, t) $ を書き換えると、次のようになります。\n",
    "\n",
    "$$\n",
    "\\mu_{\\theta}(x_t, t) = \\frac{1}{\\sqrt{\\alpha_t}} \\left( x_t - \\frac{1 - \\alpha_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\, \\varepsilon_{\\theta}(x_t, t) \\right)\n",
    "$$\n",
    "\n",
    "ここでは $ \\varepsilon_{\\theta}(x_t, t) $ がニューラルネットワークの出力を表します。このとき KL ダイバージェンスは次のように計算できます。\n",
    "\n",
    "$$\n",
    "D_{KL}(q(x_{t-1} \\mid x_t, x_0) \\parallel p_{\\theta}(x_{t-1} \\mid x_t)) = \\frac{1}{2 \\sigma_q^2(t)} \\| \\mu_{\\theta}(x_t, t) - \\mu_q(x_t, x_0) \\|^2\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\frac{1}{2 \\sigma_q^2(t)} \\left( \\frac{1 - \\alpha_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\right)^2 \\| \\varepsilon_{\\theta}(x_t, t) - \\varepsilon \\|^2\n",
    "$$\n",
    "\n",
    "よって、NNは元データから$x_{t}$を作るときに使うノイズ成分$\\epsilon$を予測することになる。\n",
    "\n",
    "![](./picture/pict28.png)\n",
    "\n",
    "疑似コードは以下のようになる\n",
    "\n",
    "![](./picture/pict29.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.学習後のデータのサンプリング\n",
    "\n",
    "$ p_{\\theta}(x_{t-1} \\mid x_t)  $は次の式で表せれた\n",
    "\n",
    "$$\n",
    "p_{\\theta}(x_{t-1} \\mid x_t) = \\mathcal{N}(x_{t-1}; \\mu_{\\theta}(x_t, t), \\sigma_q^2(t) \\mathbf{I})\n",
    "$$\n",
    "\n",
    "これより $ x_{t-1} \\sim p_{\\theta}(x_{t-1} \\mid x_t) $ によるサンプリングは、変数変換トリックを使うと次の式で表されます。\n",
    "\n",
    "$$\n",
    "\\varepsilon \\sim \\mathcal{N}(0, \\mathbf{I})\n",
    "$$\n",
    "\n",
    "$$\n",
    "x_{t-1} = \\mu_{\\theta}(x_t, t) + \\sigma_q(t) \\varepsilon\n",
    "$$\n",
    "\n",
    "ここで $ \\mu_{\\theta}(x_t, t) $ と$ \\sigma_q(t) $ は次の式で表されます。\n",
    "\n",
    "$$\n",
    "\\mu_{\\theta}(x_t, t) = \\frac{1}{\\sqrt{\\alpha_t}} \\left( x_t - \\frac{1 - \\alpha_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\, \\varepsilon_{\\theta}(x_t, t) \\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\sigma_q(t) = \\sqrt{\\frac{(1 - \\alpha_t)(1 - \\bar{\\alpha}_{t-1})}{1 - \\bar{\\alpha}_t}}\n",
    "$$\n",
    "\n",
    "これを時刻 $T$ からスタートして、1 つずつ前の時刻のデータを生成します。\n",
    "\n",
    "また最後の時刻の処理$ x_1 $ から $ x_0 $ を生成する処理）は、ノイズの追加をなくすとよい結果が得られることがわかっています。そこで $ t = 1 $ のときは $ \\varepsilon = 0 $ として、ノイズを追加しないようにします。以上のデータ生成のアルゴリズムは擬似コードで次のように表されます。\n",
    "\n",
    "![](./picture/pict30.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
